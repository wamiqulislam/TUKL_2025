{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3549868,"sourceType":"datasetVersion","datasetId":2134482}],"dockerImageVersionId":30198,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import time\nimport torch.utils.data\nimport os\nimport sys\nimport rasterio\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nimport random\nimport re\nimport numpy\nfrom tqdm import tqdm\n\nfrom math import cos,pi\nfrom sklearn.metrics import f1_score, precision_score, recall_score, jaccard_score, accuracy_score, confusion_matrix\nfrom scipy.ndimage import morphology\nfrom scipy.ndimage.filters import maximum_filter1d\nfrom torch.nn import Module, Sequential\nfrom torch.nn import Conv3d, ConvTranspose3d, BatchNorm3d, MaxPool3d, AvgPool1d, Dropout3d\nfrom torch.nn import ReLU, Sigmoid\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\nimport os\nimport rasterio\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfrom tqdm import tqdm\nimport numpy as np\nfrom collections import defaultdict\nimport seaborn as sns\nimport pandas as pd\nimport math\n\nimport os\nimport numpy as np\nimport torch\nimport rasterio\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\n\nfrom scipy.stats import mode\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:26:52.805137Z","iopub.execute_input":"2025-07-18T06:26:52.805617Z","iopub.status.idle":"2025-07-18T06:26:55.172038Z","shell.execute_reply.started":"2025-07-18T06:26:52.805499Z","shell.execute_reply":"2025-07-18T06:26:55.171338Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/sentinel2-crop-mapping'\nFOLDERS = ['lombardia', 'lombardia2']\nYEARS = ['data2016', 'data2017', 'data2018']\nIMG_SIZE = (48, 48)\nBANDS = 9\n\ndef is_valid_image(file):\n    return file.endswith('.tif') and '_MSAVI' not in file and os.path.basename(file) != 'y.tif'\n\ndef load_image_stack(tile_path):\n    # Load and stack all satellite images (ignore MSAVI)\n    img_files = sorted([f for f in os.listdir(tile_path) if is_valid_image(f)])\n    stack = []\n    for f in img_files:\n        with rasterio.open(os.path.join(tile_path, f)) as src:\n            arr = src.read()  # shape: (9, 48, 48)\n            stack.append(arr)\n    return np.stack(stack)  # shape: (T, 9, 48, 48)\n\ndef load_mask(tile_path):\n    y_path = os.path.join(tile_path, 'y.tif')\n    with rasterio.open(y_path) as src:\n        mask = src.read(1)\n    return remap_labels(mask, label_remap) #(48,48) image with remapped labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:26:55.173490Z","iopub.execute_input":"2025-07-18T06:26:55.173953Z","iopub.status.idle":"2025-07-18T06:26:55.180843Z","shell.execute_reply.started":"2025-07-18T06:26:55.173925Z","shell.execute_reply":"2025-07-18T06:26:55.180061Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"label_remap = {\n    2: 0,  # Cereals\n    9: 1,  # Maize\n    12: 2, # Rice\n    7: 3,  # Forage\n    1: 4, 3: 4, 5: 4, 6: 4, 8: 4, 10: 4, 11: 4, 13: 4, 14: 4, 15: 4, 16: 4, 19: 4, 255:4,  # Unknown crop\n    4: 5,  # Woods/tree crops\n    17: 6, 18: 6, 20: 6, 21: 6,  # Non-agricultural\n}\n\ndef remap_labels(mask, label_remap):\n    return np.vectorize(lambda x: label_remap.get(x, 255))(mask).astype(np.uint8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:26:55.181972Z","iopub.execute_input":"2025-07-18T06:26:55.182342Z","iopub.status.idle":"2025-07-18T06:26:55.195870Z","shell.execute_reply.started":"2025-07-18T06:26:55.182304Z","shell.execute_reply":"2025-07-18T06:26:55.195086Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"all_data = []\n\nfor region in FOLDERS:\n    for year in YEARS:\n        tiles_root = os.path.join(BASE_PATH, region, year)\n        tile_ids = sorted(os.listdir(tiles_root))\n\n        for tile_id in tqdm(tile_ids[:10], desc=f\"{region}/{year}\"):  # ‚Üê change [:10] to full load\n            tile_path = os.path.join(tiles_root, tile_id)\n            try:\n                X = load_image_stack(tile_path)\n                y = load_mask(tile_path)\n                all_data.append((X, y))\n            except Exception as e:\n                print(f\"‚ùå Failed to load {tile_path}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:26:55.196772Z","iopub.execute_input":"2025-07-18T06:26:55.197087Z","iopub.status.idle":"2025-07-18T06:27:31.368143Z","shell.execute_reply.started":"2025-07-18T06:26:55.197047Z","shell.execute_reply":"2025-07-18T06:27:31.367424Z"}},"outputs":[{"name":"stderr","text":"lombardia/data2016: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.56it/s]\nlombardia/data2017: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.69it/s]\nlombardia/data2018: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:07<00:00,  1.27it/s]\nlombardia2/data2016: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.65it/s]\nlombardia2/data2017: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.62it/s]\nlombardia2/data2018: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.20it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"label_counter = Counter()\n\nfor _, y in all_data:\n    flat_labels = y.flatten()\n    label_counter.update(flat_labels.tolist())\n\n# Sort the label counts\nlabel_counts = dict(sorted(label_counter.items()))\n\n# ‚úÖ Total number of unique labels\nnum_labels = len(label_counts)\n\n# ‚úÖ Total number of labeled pixels\ntotal_pixels = sum(label_counts.values())\n\n# üîç Print everything\nprint(f\"üî¢ Total unique labels: {num_labels}\")\nprint(f\"üßÆ Total labeled pixels: {total_pixels}\")\nprint(\"üìä Label Frequencies:\")\nfor label, count in label_counts.items():\n    print(f\"  Label {label}: {count} pixels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.369955Z","iopub.execute_input":"2025-07-18T06:27:31.370224Z","iopub.status.idle":"2025-07-18T06:27:31.385849Z","shell.execute_reply.started":"2025-07-18T06:27:31.370199Z","shell.execute_reply":"2025-07-18T06:27:31.385083Z"}},"outputs":[{"name":"stdout","text":"üî¢ Total unique labels: 7\nüßÆ Total labeled pixels: 138240\nüìä Label Frequencies:\n  Label 0: 23334 pixels\n  Label 1: 22003 pixels\n  Label 2: 33120 pixels\n  Label 3: 5191 pixels\n  Label 4: 21574 pixels\n  Label 5: 19636 pixels\n  Label 6: 13382 pixels\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#fixed seed\nnp.random.seed(13)\n\n#80/20 split\ntotal_len = len(all_data)\ntrain_len = int(0.8 * total_len)\nval_len = total_len - train_len\ntrain_raw, val_raw = random_split(all_data, [train_len, val_len])\n\nprint(f\"Total samples: {len(all_data)}\")\nprint(f\"Train samples: {len(train_raw)}\")\nprint(f\"Test samples:  {len(val_raw)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.386793Z","iopub.execute_input":"2025-07-18T06:27:31.387033Z","iopub.status.idle":"2025-07-18T06:27:31.399787Z","shell.execute_reply.started":"2025-07-18T06:27:31.387011Z","shell.execute_reply":"2025-07-18T06:27:31.399083Z"}},"outputs":[{"name":"stdout","text":"Total samples: 60\nTrain samples: 48\nTest samples:  12\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from collections import defaultdict\n\n# Assume train_raw and/or val_raw is a list of (X, y) where X.shape = (T, 9, 48, 48)\n# This works for any dataset (train_raw, val_raw, test_raw, etc.)\ndef analyze_timesteps(data):\n    timestep_counts = defaultdict(int)\n\n    for X, _ in data:\n        T = X.shape[0]\n        timestep_counts[T] += 1\n\n    # Print detailed info\n    print(\"üìä Unique Time Step Counts:\")\n    for T in sorted(timestep_counts):\n        print(f\"  T = {T}: {timestep_counts[T]} tiles\")\n\n    print(f\"\\nüßÆ Total Unique T values: {len(timestep_counts)}\")\n    return timestep_counts\n\n# Example usage\nanalyze_timesteps(train_raw)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.401555Z","iopub.execute_input":"2025-07-18T06:27:31.401829Z","iopub.status.idle":"2025-07-18T06:27:31.411025Z","shell.execute_reply.started":"2025-07-18T06:27:31.401785Z","shell.execute_reply":"2025-07-18T06:27:31.410327Z"}},"outputs":[{"name":"stdout","text":"üìä Unique Time Step Counts:\n  T = 33: 6 tiles\n  T = 34: 8 tiles\n  T = 53: 19 tiles\n  T = 68: 7 tiles\n  T = 71: 8 tiles\n\nüßÆ Total Unique T values: 5\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"defaultdict(int, {53: 19, 33: 6, 68: 7, 71: 8, 34: 8})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def plot_sample(X, y, timestep=0, band=3):\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(X[timestep, band], cmap='gray')\n    axs[0].set_title(f'Band {band} @ t={timestep}')\n    axs[1].imshow(y, cmap='tab20')\n    axs[1].set_title('Ground Truth')\n    plt.show()\n\n# Plot a sample\nsample_X, sample_y = all_data[20]\nprint(f\"Image sequence shape: {sample_X.shape}\")  # (T, 9, 48, 48)\nprint(f\"Ground truth shape: {sample_y.shape}\")    # (48, 48)\nplot_sample(sample_X, sample_y, timestep=0, band=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.411956Z","iopub.execute_input":"2025-07-18T06:27:31.412270Z","iopub.status.idle":"2025-07-18T06:27:31.693124Z","shell.execute_reply.started":"2025-07-18T06:27:31.412238Z","shell.execute_reply":"2025-07-18T06:27:31.692412Z"}},"outputs":[{"name":"stdout","text":"Image sequence shape: (68, 9, 48, 48)\nGround truth shape: (48, 48)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x360 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":8},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class CNNEncoder(nn.Module):\n    def __init__(self, in_channels=9, out_channels=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):  # x: (B, 9, 48, 48)\n        return self.net(x)  # (B, C, 48, 48)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.694188Z","iopub.execute_input":"2025-07-18T06:27:31.694807Z","iopub.status.idle":"2025-07-18T06:27:31.700241Z","shell.execute_reply.started":"2025-07-18T06:27:31.694762Z","shell.execute_reply":"2025-07-18T06:27:31.699421Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class CNN_LSTM_Segmentation(nn.Module):\n    def __init__(self, cnn_feat_dim=64, lstm_hidden=64, num_classes=7):\n        super().__init__()\n        self.cnn = CNNEncoder()\n        self.lstm = nn.LSTM(input_size=cnn_feat_dim, hidden_size=lstm_hidden,\n                            batch_first=True, bidirectional=True)\n        self.classifier = nn.Conv2d(lstm_hidden*2, num_classes, kernel_size=1)\n\n    def forward(self, x):  # x: (B, T, C, H, W)\n        B, T, C, H, W = x.shape\n\n    # Extract spatial features from each image\n        feat_seq = []\n        for t in range(T):\n            feat = self.cnn(x[:, t])  # (B, feat_dim, H, W)\n            feat_seq.append(feat)\n        feat_seq = torch.stack(feat_seq, dim=1)  # (B, T, feat_dim, H, W)\n\n    # Prepare for LSTM\n        feat_seq = feat_seq.permute(0, 3, 4, 1, 2)  # (B, H, W, T, feat_dim)\n        feat_seq = feat_seq.reshape(B * H * W, T, -1)  # (B*H*W, T, feat_dim)\n\n        lstm_out, _ = self.lstm(feat_seq)  # (B*H*W, T, 2H)\n        lstm_last = lstm_out[:, -1, :]     # (B*H*W, 2H) seleetcing last timestep\n\n        \n        lstm_last = lstm_last.view(B, H, W, -1).permute(0, 3, 1, 2)  # (B, 128, H, W)\n        out = self.classifier(lstm_last)   # (B, num_classes, H, W)\n          # Conv2d: (B, num_classes, H, W)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.701247Z","iopub.execute_input":"2025-07-18T06:27:31.701531Z","iopub.status.idle":"2025-07-18T06:27:31.820345Z","shell.execute_reply.started":"2025-07-18T06:27:31.701509Z","shell.execute_reply":"2025-07-18T06:27:31.819518Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class TemporalPatchDataset(Dataset):\n    def __init__(self, X, y, window_T=10, stride_T=4, crop_size=16, crop_stride=16):\n        self.X = X  # shape: (T, 9, 48, 48)\n        self.y = y  # shape: (48, 48)\n        self.window_T = window_T\n        self.stride_T = stride_T\n        self.crop_size = crop_size\n        self.crop_stride = crop_stride\n        self.samples = []\n\n        T, _, H, W = X.shape\n        for t in range(0, T - window_T + 1, stride_T):\n            for i in range(0, H - crop_size + 1, crop_stride):\n                for j in range(0, W - crop_size + 1, crop_stride):\n                    self.samples.append((t, i, j))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        t, i, j = self.samples[idx]\n        x_patch = self.X[t:t+self.window_T, :, i:i+self.crop_size, j:j+self.crop_size]\n        y_patch = self.y[i:i+self.crop_size, j:j+self.crop_size]\n        return torch.tensor(x_patch, dtype=torch.float32), torch.tensor(y_patch, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:31.821445Z","iopub.execute_input":"2025-07-18T06:27:31.821783Z","iopub.status.idle":"2025-07-18T06:27:31.832034Z","shell.execute_reply.started":"2025-07-18T06:27:31.821756Z","shell.execute_reply":"2025-07-18T06:27:31.831235Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"window_size = 10\nstride = 4\ncrop_size = 16\ncrop_stride = 16\nbatch_size = 8\n\ntrain_loaders = build_patch_loaders_by_T(train_raw, window_size, stride, crop_size, crop_stride, batch_size, shuffle=True)\nval_loaders = build_patch_loaders_by_T(val_raw, window_size, stride, crop_size, crop_stride, batch_size, shuffle=False)\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CNN_LSTM_Segmentation().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nnum_epochs = 10\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []\n\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n    for T_val, group_loader in tqdm(train_loaders, desc=\"Training - Timestep Groups\"):\n        for X_tile, y_tile in group_loader:\n            X_tile = X_tile.to(device)        # (B, T, C, H, W)\n            y_tile = y_tile.to(device)        # (B, H, W)\n\n            B, T, C, H, W = X_tile.shape\n            sum_logits = None\n            count = 0\n\n            # Sliding window inference\n            for i in range(0, T - window_size + 1, stride):\n                window = X_tile[:, i:i + window_size]  # (B, window_size, C, H, W)\n                logits = model(window)                 # (B, num_classes, H, W)\n\n                if sum_logits is None:\n                    sum_logits = logits\n                else:\n                    sum_logits += logits\n                count += 1\n\n                del window, logits\n                torch.cuda.empty_cache()\n\n            avg_logits = sum_logits / count\n            loss = criterion(avg_logits, y_tile)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                preds = avg_logits.argmax(dim=1)  # (B, H, W)\n                train_correct += (preds == y_tile).sum().item()\n                train_total += y_tile.numel()\n\n            train_loss += loss.item() * B\n\n            del X_tile, y_tile, sum_logits, avg_logits, preds, loss\n            torch.cuda.empty_cache()\n\n    avg_train_loss = train_loss / train_total\n    train_acc = train_correct / train_total\n    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n\n    # ------ Validation ------\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    for T_val, group_loader in tqdm(val_loaders, desc=\"Validation - Timestep Groups\"):\n        for X_tile, y_tile in group_loader:\n            X_tile = X_tile.to(device)\n            y_tile = y_tile.to(device)\n\n            B, T, C, H, W = X_tile.shape\n            sum_logits = None\n            count = 0\n\n            with torch.no_grad():\n                for i in range(0, T - window_size + 1, stride):\n                    window = X_tile[:, i:i + window_size]\n                    logits = model(window)\n\n                    if sum_logits is None:\n                        sum_logits = logits\n                    else:\n                        sum_logits += logits\n                    count += 1\n\n                    del window, logits\n                    torch.cuda.empty_cache()\n\n                avg_logits = sum_logits / count\n                loss = criterion(avg_logits, y_tile)\n                preds = avg_logits.argmax(dim=1)\n\n                val_correct += (preds == y_tile).sum().item()\n                val_total += y_tile.numel()\n                val_loss += loss.item() * B\n\n            del X_tile, y_tile, sum_logits, avg_logits, preds, loss\n            torch.cuda.empty_cache()\n\n    avg_val_loss = val_loss / val_total\n    val_acc = val_correct / val_total\n    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:33:58.691235Z","iopub.execute_input":"2025-07-18T06:33:58.692096Z","iopub.status.idle":"2025-07-18T06:36:43.233985Z","shell.execute_reply.started":"2025-07-18T06:33:58.692056Z","shell.execute_reply":"2025-07-18T06:36:43.232920Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2857\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0089 | Val Acc: 0.1557\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2792\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0085 | Val Acc: 0.1557\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2864\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0086 | Val Acc: 0.1557\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.77s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2801\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0088 | Val Acc: 0.1557\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2887\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0084 | Val Acc: 0.1557\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.78s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2823\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0087 | Val Acc: 0.1557\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2842\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0085 | Val Acc: 0.1557\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2846\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0083 | Val Acc: 0.1557\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2822\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0083 | Val Acc: 0.1557\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070 | Train Acc: 0.2858\n","output_type":"stream"},{"name":"stderr","text":"Validation - Timestep Groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0082 | Val Acc: 0.1557\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Evalutaion","metadata":{}},{"cell_type":"code","source":"def visualize(model, dataset, idx=0, num_classes=7, label_names=None, window_size=25, stride=1):\n    model.eval()\n    \n    # Get one sample\n    X, y = dataset[idx]\n    X = torch.tensor(X).float().to(device)      # (T, 9, 48, 48)\n    y = torch.tensor(y).long().numpy()          # (48, 48)\n\n    # Sliding window inference\n    T = X.shape[0]\n    logits_sum = torch.zeros((num_classes, 48, 48), device=device)\n    count = 0\n\n    for start in range(0, T - window_size + 1, stride):\n        window = X[start:start + window_size]   # (window_size, 9, 48, 48)\n        window = window.unsqueeze(0)            # Add batch dim ‚Üí (1, T, 9, 48, 48)\n\n        with torch.no_grad():\n            logits = model(window)[0]           # (C, 48, 48)\n        logits_sum += logits\n        count += 1\n\n    pred = logits_sum.argmax(0).cpu().numpy()   # (48, 48)\n\n    # Plotting\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    axs[0].imshow(y, cmap='tab20')\n    axs[0].set_title(\"Ground Truth\")\n\n    axs[1].imshow(pred, cmap='tab20')\n    axs[1].set_title(\"Prediction\")\n\n    axs[2].imshow(y == pred, cmap='gray')\n    axs[2].set_title(\"Correct Prediction Mask\")\n\n    for ax in axs:\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\ndef evaluate_full_dataset(model, dataset, num_classes=7, label_names=None, window_size=25, stride=1):\n    model.eval()\n    total_cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n\n    with torch.no_grad():\n        for X, y in tqdm(dataset, desc=\"Evaluating\"):\n            X = torch.tensor(X).float().to(device)   # (T, 9, 48, 48)\n            y_np = torch.tensor(y).long().numpy().flatten()  # (48*48,)\n\n            T = X.shape[0]\n            logits_sum = torch.zeros((num_classes, 48, 48), device=device)\n            count = 0\n\n            for start in range(0, T - window_size + 1, stride):\n                window = X[start:start + window_size]  # (window_size, 9, 48, 48)\n                window = window.unsqueeze(0)           # (1, T, 9, 48, 48)\n                logits = model(window)[0]              # (C, 48, 48)\n                logits_sum += logits\n                count += 1\n\n            pred = logits_sum.argmax(0).cpu().numpy().flatten()  # (48*48,)\n            cm = confusion_matrix(y_np, pred, labels=np.arange(num_classes))\n            total_cm += cm\n\n    acc = np.trace(total_cm) / np.sum(total_cm)\n    print(\"Pixel-wise Accuracy:\", acc)\n\n    if label_names:\n        print(\"\\nClass-wise IoU:\")\n        for i in range(num_classes):\n            TP = total_cm[i, i]\n            FP = total_cm[:, i].sum() - TP\n            FN = total_cm[i, :].sum() - TP\n            denom = TP + FP + FN\n            iou = TP / denom if denom > 0 else 0\n            print(f\"{label_names[i]}: {iou:.3f}\")\n\n\n\n\nlabel_names = {\n    0: \"Cereals\",\n    1: \"Maize\",\n    2: \"Rice\",\n    3: \"Forage\",\n    4: \"Unknown crop\",\n    5: \"Woods/tree crops\",\n    6: \"Non-agricultural\"\n}\n\nvisualize(model, train_raw, idx=9, num_classes=7, label_names=label_names)\nevaluate_full_dataset(model,  train_raw, num_classes=7, label_names=label_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:46:11.089766Z","iopub.execute_input":"2025-07-18T06:46:11.090171Z","iopub.status.idle":"2025-07-18T06:46:28.486355Z","shell.execute_reply.started":"2025-07-18T06:46:11.090139Z","shell.execute_reply":"2025-07-18T06:46:28.485573Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x288 with 3 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"name":"stderr","text":"Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:16<00:00,  2.89it/s]","output_type":"stream"},{"name":"stdout","text":"Pixel-wise Accuracy: 0.1766673900462963\n\nClass-wise IoU:\nCereals: 0.177\nMaize: 0.000\nRice: 0.000\nForage: 0.000\nUnknown crop: 0.000\nWoods/tree crops: 0.000\nNon-agricultural: 0.000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Loading Model","metadata":{}},{"cell_type":"code","source":"epoch = 10\nmodel.load_state_dict(torch.load(f\"model_epoch_{epoch}.pth\"))\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T06:27:36.169714Z","iopub.status.idle":"2025-07-18T06:27:36.170172Z","shell.execute_reply.started":"2025-07-18T06:27:36.169941Z","shell.execute_reply":"2025-07-18T06:27:36.169965Z"}},"outputs":[],"execution_count":null}]}